# üìàüìä ECOMMERCE SALES DATASET üõí üíµ



## Sum√°rio

- [OBJETIVO](#1-objetivo)
- [DATA ARCHITECTURE](#2-data-architecture)
- [DATASET DIAGRAM](#3-dataset-diagram)
- [MEDALLION ARCHITECTURE](#4-medallion-architecture)
- [PIPELINE](#5-pipeline)
- [ANALYSIS](#6-analysis)
- [TECH STACK](#7-tech-stack)

<hr>

### 1. OBJETIVO
O objetivo do desafio √© montar uma arquitetura medalh√£o (bronze, silver,gold) utilizando o databricks, a fonte de dados deve ser um conjunto de dados do Kaggle relacionado √† vendas. A disponibiliza√ß√£o dos dados da gold devem ser realizadas em formato parquet e delta (aproveitando funcionalidades de versionamento e transa√ß√µes ACID).

A Base de dados utilizada foi a [Ecommerce Sales Dataset no Kaggle](https://www.kaggle.com/datasets/thedevastator/unlock-profits-with-e-commerce-sales-data)

Foi realizado tamb√©m algumas consultas explorat√≥rias diponibilizadas na pasta:

[Analysis](04.analysis/)</p>


### 2. DATA ARCHITECTURE

![Project Architecture](06.img/Arquitetura_Ecommerce_Sales_Dataset.png)

<hr>

### 3. DATASET DIAGRAM

![Project Architecture](06.img/Ecommerce_Sales_Dataset_Diagram.png)

<hr>

### 4. MEDALLION ARCHITECTURE

#### CAMADA BRONZE
Realizar a ingest√£o e o tratamento inicial de um arquivo CSV de relat√≥rios de vendas da Amazon, armazenado em um Data Lake no Azure Databricks. Abaixo est√£o as principais etapas e objetivos:

##### 1. Listagem de Arquivos no Diret√≥rio Landing
O comando `%fs ls 'dbfs:/mnt/datalakeeccomerceproject/prod/landing/'` lista os arquivos no diret√≥rio de landing onde os dados brutos s√£o inicialmente armazenados. Isso serve como uma verifica√ß√£o para garantir que os arquivos esperados est√£o dispon√≠veis antes de iniciar o processo de ingest√£o.

##### 2. Importa√ß√£o de Bibliotecas
O c√≥digo importa as bibliotecas necess√°rias, como pyspark.sql.functions para manipula√ß√£o de dados, pyspark.sql.types para definir o esquema do DataFrame, e delta para trabalhar com o formato Delta Lake.

##### 3. Defini√ß√£o do Esquema do DataFrame
Foi definido um esquema expl√≠cito (StructType) para o DataFrame, que mapeia as colunas do CSV para tipos espec√≠ficos do Spark:

StringType √© usado para a maioria das colunas, pois os dados brutos s√£o lidos como texto.
Este esquema garante que os dados sejam carregados corretamente, com cada coluna associada ao tipo de dado apropriado.

##### 4. Leitura do Arquivo CSV
O arquivo CSV √© carregado a partir do caminho `dbfs:/mnt/datalakeeccomerceproject/prod/landing/Amazon Sale Report.csv` usando o esquema definido:

spark.read.format("csv"): Especifica que o formato do arquivo de origem √© CSV.
.option("header", "true"): Indica que o arquivo CSV cont√©m um cabe√ßalho.
.schema(df_schema): Aplica o esquema customizado durante a leitura dos dados.

##### 5. Escrita dos Dados no N√≠vel Bronze
Os dados lidos s√£o ent√£o escritos no Data Lake como uma tabela no n√≠vel bronze, utilizando o formato Delta:

`.write.format("delta").mode("overwrite").save("/mnt/datalakeeccomerceproject/prod/bronze/tb_amz_sales_report")`: Esse comando escreve o DataFrame no formato Delta, sobrescrevendo qualquer dado existente na tabela `tb_amz_sales_report`.

O n√≠vel bronze armazena dados brutos com m√≠nimas transforma√ß√µes, servindo como a camada de armazenamento inicial dos dados ap√≥s a ingest√£o.

<hr>

#### CAMADA SILVER

##### 1. Sele√ß√£o e Renomea√ß√£o de Colunas
A primeira parte do c√≥digo seleciona e renomeia colunas da tabela bronze sales_bronze.tb_amz_sales_report para preparar os dados no n√≠vel silver. Algumas colunas foram renomeadas para padroniza√ß√£o, enquanto outras foram transformadas para garantir a consist√™ncia dos dados. Por exemplo:

- index foi renomeado para id.
- A coluna date foi convertida de string para o tipo de data (Order_Date).
- A coluna Status foi renomeada para Order_Status.

##### 2. Tratamento de Valores Nulos e Ajustes Espec√≠ficos
Em seguida, o c√≥digo realiza o tratamento de valores nulos utilizando a fun√ß√£o coalesce. Esse tratamento √© feito para evitar problemas em an√°lises futuras. Colunas como Courier_Status, Currency, Amount, entre outras, foram tratadas para garantir que os valores nulos sejam substitu√≠dos por valores padr√£o, como 'N/A' ou 0.

Al√©m disso, foi adicionada uma coluna Price, que calcula o pre√ßo unit√°rio de cada item com base na quantidade (Quantity) e no valor total (Amount).

##### 3. Ajuste de Estados (Ship_State)
Nessa etapa, o c√≥digo substitui siglas de estados espec√≠ficos por seus nomes completos, por exemplo:

- 'RJ' foi substitu√≠do por 'RAJSHTHAN'.
- 'NL' foi substitu√≠do por 'NAGALAND'.

Este ajuste foi feito para garantir a padroniza√ß√£o dos dados relacionados aos estados.

##### 4. Escrita dos Dados no N√≠vel Silver
Finalmente, o DataFrame tratado √© salvo em um formato Delta no n√≠vel silver, substituindo qualquer arquivo existente no caminho especificado. O uso do formato Delta facilita a versionamento dos dados e otimiza as consultas futuras.

<hr>

#### CAMADA GOLD 

Este notebook no Databricks √© projetado para transformar dados de vendas em dimens√µes e uma tabela de fatos a partir dos dados processados no n√≠vel silver. Os dados s√£o ent√£o armazenados em duas formas diferentes no n√≠vel gold: no formato Delta e Parquet. A seguir, as principais etapas e objetivos do c√≥digo s√£o explicados.

##### 1. Importa√ß√£o de Bibliotecas
O c√≥digo come√ßa com a importa√ß√£o da biblioteca pyspark.sql.functions como fn, que fornece fun√ß√µes para manipula√ß√£o de DataFrames no PySpark.

##### 2. Consulta √† Tabela Silver
Uma consulta SQL √© executada para visualizar os dados presentes na tabela `sales_silver.tb_amz_sales_report`. Esta tabela silver cont√©m dados processados e limpos, prontos para serem transformados em dimens√µes e fatos.

##### 3. Cria√ß√£o das Dimens√µes
S√£o criados v√°rios DataFrames que representam diferentes dimens√µes, extraindo colunas espec√≠ficas da tabela silver:

- Dimens√£o de Pedidos (df_dim_orders): Cont√©m informa√ß√µes relacionadas aos pedidos, como Order_ID, Order_Date, Order_Status, Fulfilment, Sales_Channel, B2B, e Currency.

- Dimens√£o de Itens (df_dim_itens): Cont√©m informa√ß√µes dos itens, como SKU, Style, Category, Size, ASIN, e Price.

- Dimens√£o de Envio (df_dim_shipment): Cont√©m dados de envio, incluindo Order_ID, Ship_Status, Ship_City, Ship_State, Ship_Postal_Code, e Ship_Country.

- Dimens√£o de Entrega (df_dim_delivery): Cont√©m o status de entrega associado ao Order_ID.

##### 4. Cria√ß√£o da Tabela de Fatos
A tabela de fatos (df_fact_transactions) √© criada para capturar transa√ß√µes com as seguintes colunas:

- Transaction_ID: Um identificador √∫nico para cada transa√ß√£o.
- Order_ID: Associado ao pedido correspondente.
- SKU: Identificador do item.
- Quantity: Quantidade do item na transa√ß√£o.
- Amount: Valor da transa√ß√£o.

##### 5. Escrita dos Dados no Formato Delta
Os DataFrames que representam as dimens√µes e a tabela de fatos s√£o gravados em um Data Lake no formato Delta. Esse formato oferece vantagens como ACID compliance, hist√≥rico de vers√µes dos dados e melhor desempenho em consultas:

Cada dimens√£o e a tabela de fatos s√£o salvas no diret√≥rio `/mnt/datalakeeccomerceproject/prod/gold/` correspondente, utilizando o formato Delta.

##### 6. Escrita dos Dados no Formato Parquet
Al√©m do formato Delta, os mesmos DataFrames s√£o tamb√©m gravados no formato Parquet:

Esse processo cria uma vers√£o dos dados em um formato leve e eficiente, com compress√£o colunar, o que facilita consultas r√°pidas e armazenamento otimizado.

<hr>

### 5. PIPELINE

##### 1. Pipeline Running with Success
![Project Architecture](06.img/pipeline_ecommerce_sales_dataset.png)

##### 1. Pipeline
![Project Architecture](06.img/pipeline1_ecommerce_sales_dataset.png)


### 6. ANALYSIS
- Objetivo: Comparar o desempenho das vendas entre clientes B2B e B2C.
![Project Architecture](06.img/b2b_b2c_analysis_graph.png)

<br>

 - Objetivo: Avaliar qual a prefer√™ncia de tipo de entrega escolhido pelos clientes por estado
   
![Project Architecture](06.img/delivery_type_performance_analysis.png)



### 7. TECH STACK  
<img src="06.img/azure.png" alt="azure" style="vertical-align:top; margin:4px; height:40px; width:40px"><img src="06.img/python.png" alt="python" style="vertical-align:top; margin:4px; height:40px; width:40px"><img src="06.img/databricks.png" alt="databricks" style="vertical-align:top; margin:4px; height:40px; width:40px"><img src="06.img/sql.png" alt="sql" style="vertical-align:top; margin:4px; height:40px; width:40px"><img src="06.img/data-quality.png" alt="data-quality" style="vertical-align:top; margin:4px; height:40px; width:40px"><img src="06.img/apache_spark.png" alt="spark" style="vertical-align:top; margin:4px; height:60px; width:90px"><img src="06.img/deltalake-logo.png" alt="deltalake" style="vertical-align:top; margin:4px; height:60px; width:90px">

